{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38254/2789912978.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.992\n",
      "Model:                            OLS   Adj. R-squared:                  0.992\n",
      "Method:                 Least Squares   F-statistic:                 3.376e+05\n",
      "Date:                Tue, 04 Mar 2025   Prob (F-statistic):               0.00\n",
      "Time:                        15:06:51   Log-Likelihood:                -70605.\n",
      "No. Observations:               63058   AIC:                         1.413e+05\n",
      "Df Residuals:                   63033   BIC:                         1.415e+05\n",
      "Df Model:                          24                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0317      0.005      6.986      0.000       0.023       0.041\n",
      "x1            -0.1288      0.004    -32.606      0.000      -0.137      -0.121\n",
      "x2             0.0669      0.007     10.283      0.000       0.054       0.080\n",
      "x3             0.0571      0.007      8.707      0.000       0.044       0.070\n",
      "x4             0.0381      0.007      5.793      0.000       0.025       0.051\n",
      "x5             0.0081      0.007      1.239      0.216      -0.005       0.021\n",
      "x6             0.0125      0.007      1.905      0.057      -0.000       0.025\n",
      "x7            -0.0164      0.007     -2.489      0.013      -0.029      -0.003\n",
      "x8             0.0062      0.007      0.939      0.348      -0.007       0.019\n",
      "x9            -0.0069      0.007     -1.056      0.291      -0.020       0.006\n",
      "x10            0.0148      0.007      2.256      0.024       0.002       0.028\n",
      "x11            0.0029      0.007      0.436      0.663      -0.010       0.016\n",
      "x12            0.0129      0.007      1.969      0.049    5.75e-05       0.026\n",
      "x13           -0.0018      0.007     -0.272      0.786      -0.015       0.011\n",
      "x14           -0.0033      0.007     -0.502      0.616      -0.016       0.010\n",
      "x15           -0.0180      0.007     -2.731      0.006      -0.031      -0.005\n",
      "x16           -0.0029      0.007     -0.437      0.662      -0.016       0.010\n",
      "x17            0.0157      0.007      2.382      0.017       0.003       0.029\n",
      "x18           -0.0016      0.007     -0.239      0.811      -0.014       0.011\n",
      "x19           -0.0006      0.007     -0.092      0.927      -0.013       0.012\n",
      "x20           -0.0062      0.007     -0.949      0.343      -0.019       0.007\n",
      "x21           -0.0471      0.007     -7.163      0.000      -0.060      -0.034\n",
      "x22           -0.0900      0.007    -13.716      0.000      -0.103      -0.077\n",
      "x23           -0.2163      0.007    -33.219      0.000      -0.229      -0.204\n",
      "x24            1.3010      0.004    329.389      0.000       1.293       1.309\n",
      "==============================================================================\n",
      "Omnibus:                    11270.669   Durbin-Watson:                   2.033\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           181803.608\n",
      "Skew:                          -0.388   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.282   Cond. No.                         197.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('jena_climate_2009_2016.csv')\n",
    "\n",
    "time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "series = df['T (degC)'][5::6]\n",
    "series.index = time[5::6]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# function for generating the lagged matrix\n",
    "def split_sequence(sequence, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    # for all indexes\n",
    "    for i in range(len(sequence)):\n",
    "        end_idx = i + window_size\n",
    "        # exit condition\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break\n",
    "        # get X and Y values\n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "train = series[:-int(len(series)/10)]\n",
    "test = series[-int(len(series)/10):]\n",
    "X_train, y_train = split_sequence(train, window_size=24)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# train Ordinary Least Squares model\n",
    "X_train = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train, X_train)\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9747823877305761\n",
      "Testing Score: 0.894765216952948\n",
      "   Actual %_Weight_Loss  Predicted %_Weight_Loss\n",
      "0                   0.0                      0.0\n",
      "1                   0.0                      0.0\n",
      "2                   0.0                      0.0\n",
      "3                   0.0                      0.0\n",
      "4                   0.0                      0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"resources/color_texture_weight_data.csv\")\n",
    "\n",
    "# Define selected features\n",
    "selected_features = [\n",
    "    \"Mean_RGB_R\", \"Mean_RGB_G\", \"Mean_RGB_B\", \"Std_RGB_R\", \"Std_RGB_G\", \"Std_RGB_B\",\n",
    "    \"Mean_LAB_L\", \"Mean_LAB_A\", \"Mean_LAB_B\", \"Std_LAB_L\", \"Std_LAB_A\", \"Std_LAB_B\",\n",
    "    \"Mean_HSV_H\", \"Mean_HSV_S\", \"Mean_HSV_V\", \"Std_HSV_H\", \"Std_HSV_S\", \"Std_HSV_V\",\n",
    "    \"Mean_GRAY_Gray\", \"Std_GRAY_Gray\",\n",
    "    \"GLCM_contrast\", \"GLCM_dissimilarity\", \"GLCM_homogeneity\", \"GLCM_energy\", \"GLCM_correlation\", \"GLCM_ASM\",\n",
    "    \"LBP_0\", \"LBP_1\", \"LBP_2\", \"LBP_3\", \"LBP_4\", \"LBP_5\", \"LBP_6\", \"LBP_7\", \"LBP_8\", \"LBP_9\",\n",
    "    \"Yellow\", \"Cyan\", \"Magenta\", \"Brightness\", \"Chroma\",\n",
    "    \"Day\", \"Temp\", \"Rep\"\n",
    "]\n",
    "\n",
    "# Filter dataset for Day 0 to 8\n",
    "df_filtered = df[df[\"Day\"] <= 8][selected_features + [\"%_Weight_Loss\"]]\n",
    "\n",
    "# Identify unique tracking experiments (Temp, Rep)\n",
    "unique_experiments = df_filtered[[\"Temp\", \"Rep\"]].drop_duplicates()\n",
    "\n",
    "# Split 80% training and 20% testing based on unique experiments\n",
    "train_experiments, test_experiments = train_test_split(\n",
    "    unique_experiments, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create train and test sets based on selected experiments\n",
    "df_train = df_filtered.merge(train_experiments, on=[\"Temp\", \"Rep\"])\n",
    "df_test = df_filtered.merge(test_experiments, on=[\"Temp\", \"Rep\"])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[selected_features])\n",
    "y_train = df_train[\"%_Weight_Loss\"]\n",
    "\n",
    "X_test = scaler.transform(df_test[selected_features])\n",
    "y_test = df_test[\"%_Weight_Loss\"]\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Score: {train_score}\")\n",
    "print(f\"Testing Score: {test_score}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame for actual vs predicted values\n",
    "predictions_df = pd.DataFrame({\"Actual %_Weight_Loss\": y_test.values, \"Predicted %_Weight_Loss\": y_pred})\n",
    "\n",
    "# Display predictions\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Requirement already satisfied: tensorflow[and-cuda] in ./venv/lib/python3.12/site-packages (2.18.0)',\n",
       " 'Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.1.0)',\n",
       " 'Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.6.3)',\n",
       " 'Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (25.2.10)',\n",
       " 'Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.6.0)',\n",
       " 'Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.2.0)',\n",
       " 'Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (18.1.1)',\n",
       " 'Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.4.0)',\n",
       " 'Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (24.2)',\n",
       " 'Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (5.29.3)',\n",
       " 'Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.32.3)',\n",
       " 'Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (75.8.0)',\n",
       " 'Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.17.0)',\n",
       " 'Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.5.0)',\n",
       " 'Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (4.12.2)',\n",
       " 'Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.17.2)',\n",
       " 'Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.70.0)',\n",
       " 'Requirement already satisfied: tensorboard<2.19,>=2.18 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.18.0)',\n",
       " 'Requirement already satisfied: keras>=3.5.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.8.0)',\n",
       " 'Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.0.2)',\n",
       " 'Requirement already satisfied: h5py>=3.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.13.0)',\n",
       " 'Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.4.1)',\n",
       " 'Requirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.3.2)',\n",
       " 'Requirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)',\n",
       " 'Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)',\n",
       " 'Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)',\n",
       " 'Requirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)',\n",
       " 'Requirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (9.3.0.75)',\n",
       " 'Requirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.2.3.61)',\n",
       " 'Requirement already satisfied: nvidia-curand-cu12==10.3.6.82 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (10.3.6.82)',\n",
       " 'Requirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.6.3.83)',\n",
       " 'Requirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.1.3)',\n",
       " 'Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.21.5)',\n",
       " 'Requirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in ./venv/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)',\n",
       " 'Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.45.1)',\n",
       " 'Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (13.9.4)',\n",
       " 'Requirement already satisfied: namex in ./venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.0.8)',\n",
       " 'Requirement already satisfied: optree in ./venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow[and-cuda]) (0.14.0)',\n",
       " 'Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.1)',\n",
       " 'Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.10)',\n",
       " 'Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.3.0)',\n",
       " 'Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2025.1.31)',\n",
       " 'Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.7)',\n",
       " 'Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (0.7.2)',\n",
       " 'Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.1.3)',\n",
       " 'Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.0.2)',\n",
       " 'Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (3.0.0)',\n",
       " 'Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (2.19.1)',\n",
       " 'Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[and-cuda]) (0.1.2)']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python3 -m pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 28s]\n",
      "val_loss: 11.744100570678711\n",
      "\n",
      "Best val_loss So Far: 9.604065895080566\n",
      "Total elapsed time: 00h 04m 35s\n",
      "Epoch 1/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 513ms/step - loss: 65.2426 - val_loss: 34.3081 - learning_rate: 5.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 38.3896 - val_loss: 27.2683 - learning_rate: 5.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 34.6017 - val_loss: 17.9158 - learning_rate: 5.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 27.4037 - val_loss: 17.6746 - learning_rate: 5.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.8046 - val_loss: 18.7992 - learning_rate: 5.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 13.7079 - val_loss: 16.6585 - learning_rate: 5.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26.6970 - val_loss: 14.3634 - learning_rate: 5.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.0590 - val_loss: 14.6513 - learning_rate: 5.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.3638 - val_loss: 16.6180 - learning_rate: 5.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14.7537 - val_loss: 14.6833 - learning_rate: 5.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.6451 - val_loss: 13.3197 - learning_rate: 5.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.4333 - val_loss: 12.7353 - learning_rate: 5.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11.0983 - val_loss: 13.2482 - learning_rate: 5.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.3262 - val_loss: 12.5750 - learning_rate: 5.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.3184 - val_loss: 17.3372 - learning_rate: 5.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.1454 - val_loss: 15.3994 - learning_rate: 5.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.9906 - val_loss: 12.6935 - learning_rate: 5.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.2288 - val_loss: 14.9899 - learning_rate: 5.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.3104 - val_loss: 13.6103 - learning_rate: 5.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.2247 - val_loss: 13.8974 - learning_rate: 2.5000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.9389 - val_loss: 13.1421 - learning_rate: 2.5000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 17.6820 - val_loss: 13.7611 - learning_rate: 2.5000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.8330 - val_loss: 12.3534 - learning_rate: 2.5000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.8677 - val_loss: 14.0180 - learning_rate: 2.5000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3800 - val_loss: 13.5195 - learning_rate: 2.5000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.1310 - val_loss: 12.5574 - learning_rate: 2.5000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10.7686 - val_loss: 12.4264 - learning_rate: 2.5000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 13.1932 - val_loss: 15.5792 - learning_rate: 2.5000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11.0043 - val_loss: 14.3365 - learning_rate: 1.2500e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.4395 - val_loss: 13.3720 - learning_rate: 1.2500e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.4938 - val_loss: 13.6067 - learning_rate: 1.2500e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 7.5163 - val_loss: 12.5194 - learning_rate: 1.2500e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.9760 - val_loss: 12.3131 - learning_rate: 1.2500e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.7935 - val_loss: 13.1132 - learning_rate: 1.2500e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.1160 - val_loss: 12.3286 - learning_rate: 1.2500e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.5114 - val_loss: 12.0940 - learning_rate: 1.2500e-04\n",
      "Epoch 37/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.9370 - val_loss: 12.7117 - learning_rate: 1.2500e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.7280 - val_loss: 11.9052 - learning_rate: 1.2500e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.2310 - val_loss: 12.0705 - learning_rate: 1.2500e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.6565 - val_loss: 11.3237 - learning_rate: 1.2500e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.6810 - val_loss: 11.5146 - learning_rate: 1.2500e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0972 - val_loss: 12.5645 - learning_rate: 1.2500e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.3131 - val_loss: 12.2072 - learning_rate: 1.2500e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.0345 - val_loss: 12.4391 - learning_rate: 1.2500e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10.0666 - val_loss: 11.5445 - learning_rate: 1.2500e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.3995 - val_loss: 12.3149 - learning_rate: 6.2500e-05\n",
      "Epoch 47/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.7594 - val_loss: 12.0525 - learning_rate: 6.2500e-05\n",
      "Epoch 48/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2599 - val_loss: 11.8478 - learning_rate: 6.2500e-05\n",
      "Epoch 49/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.4578 - val_loss: 12.5232 - learning_rate: 6.2500e-05\n",
      "Epoch 50/150\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.7105 - val_loss: 11.9814 - learning_rate: 6.2500e-05\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 974ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 9.4481\n",
      "Model Loss (MSE): 11.323663711547852\n",
      "R2 Score: 0.7170486851247145\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Attention\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"resources/color_texture_weight_data.csv\")\n",
    "\n",
    "# Define feature groups\n",
    "feature_groups = {\n",
    "    \"RGB Features\": [\"Mean_RGB_R\", \"Mean_RGB_G\", \"Mean_RGB_B\", \"Std_RGB_R\", \"Std_RGB_G\", \"Std_RGB_B\"],\n",
    "    \"LAB Features\": [\"Mean_LAB_L\", \"Mean_LAB_A\", \"Mean_LAB_B\", \"Std_LAB_L\", \"Std_LAB_A\", \"Std_LAB_B\"],\n",
    "    \"HSV Features\": [\"Mean_HSV_H\", \"Mean_HSV_S\", \"Mean_HSV_V\", \"Std_HSV_H\", \"Std_HSV_S\", \"Std_HSV_V\"],\n",
    "    \"Grayscale Features\": [\"Mean_GRAY_Gray\", \"Std_GRAY_Gray\"],\n",
    "    \"Texture Features\": [\"GLCM_contrast\", \"GLCM_dissimilarity\", \"GLCM_homogeneity\", \"GLCM_energy\", \"GLCM_correlation\", \"GLCM_ASM\"],\n",
    "    \"LBP Features\": [\"LBP_0\", \"LBP_1\", \"LBP_2\", \"LBP_3\", \"LBP_4\", \"LBP_5\", \"LBP_6\", \"LBP_7\", \"LBP_8\", \"LBP_9\"],\n",
    "    \"Color Space Features\": [\"Yellow\", \"Cyan\", \"Magenta\", \"Brightness\", \"Chroma\"],\n",
    "    \"Environmental Variables\": [\"Day\", \"Temp\", \"Rep\"]\n",
    "}\n",
    "\n",
    "# Process PCA for each feature group and evaluate importance\n",
    "important_feature_groups = {}\n",
    "explained_variances = {}\n",
    "df_pca_all = pd.DataFrame()\n",
    "\n",
    "for group, features in feature_groups.items():\n",
    "    df_group = df[features].dropna()\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_group)\n",
    "    \n",
    "    pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    explained_variance = sum(pca.explained_variance_ratio_)\n",
    "    explained_variances[group] = explained_variance\n",
    "    \n",
    "    if explained_variance > 0.05:  # Keep groups explaining significant variance\n",
    "        important_feature_groups[group] = features\n",
    "        pca_features = [f\"{group}_PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "        df_pca_group = pd.DataFrame(X_pca, columns=pca_features)\n",
    "        df_pca_all = pd.concat([df_pca_all, df_pca_group], axis=1)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, target_column, time_steps=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i+time_steps].values)\n",
    "        y.append(data[target_column].iloc[i+time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare dataset for LSTM\n",
    "df_pca_all[\"%_Weight_Loss\"] = df[\"%_Weight_Loss\"].values\n",
    "time_steps = 10\n",
    "X, y = create_sequences(df_pca_all, \"%_Weight_Loss\", time_steps)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(\n",
    "            hp.Int('units1', min_value=64, max_value=256, step=32), activation='relu',\n",
    "            return_sequences=True, kernel_regularizer=l2(hp.Choice('l2_1', [0.0001, 0.001, 0.01])),\n",
    "            input_shape=(time_steps, X.shape[2])\n",
    "        )),\n",
    "        Dropout(hp.Float('dropout1', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        \n",
    "        Bidirectional(LSTM(\n",
    "            hp.Int('units2', min_value=32, max_value=128, step=32), activation='relu',\n",
    "            return_sequences=True, kernel_regularizer=l2(hp.Choice('l2_2', [0.0001, 0.001, 0.01]))\n",
    "        )),\n",
    "        Dropout(hp.Float('dropout2', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        \n",
    "        LSTM(\n",
    "            hp.Int('units3', min_value=16, max_value=64, step=16), activation='relu',\n",
    "            kernel_regularizer=l2(hp.Choice('l2_3', [0.0001, 0.001, 0.01]))\n",
    "        ),\n",
    "        Dropout(hp.Float('dropout3', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        \n",
    "        Dense(1, kernel_regularizer=l2(hp.Choice('l2_dense', [0.0001, 0.001, 0.01])))\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [0.0001, 0.0005, 0.001])), loss='mse')\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model, objective='val_loss', max_trials=10, executions_per_trial=1, directory='tuning_results'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Train the LSTM model with best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=16, validation_data=(X_test, y_test), callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Model Loss (MSE): {loss}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
