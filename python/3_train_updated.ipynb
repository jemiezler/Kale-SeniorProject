{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load data | Day_Treatment_Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>R_Mean</th>\n",
       "      <th>R_Std</th>\n",
       "      <th>G_Mean</th>\n",
       "      <th>G_Std</th>\n",
       "      <th>B_Mean</th>\n",
       "      <th>B_Std</th>\n",
       "      <th>H_Mean</th>\n",
       "      <th>H_Std</th>\n",
       "      <th>S_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>V_Mean</th>\n",
       "      <th>V_Std</th>\n",
       "      <th>L_Mean</th>\n",
       "      <th>L_Std</th>\n",
       "      <th>a_Mean</th>\n",
       "      <th>a_Std</th>\n",
       "      <th>b_Mean</th>\n",
       "      <th>b_Std</th>\n",
       "      <th>Day</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158.9</td>\n",
       "      <td>202.031017</td>\n",
       "      <td>63.184791</td>\n",
       "      <td>206.361271</td>\n",
       "      <td>55.761869</td>\n",
       "      <td>195.552566</td>\n",
       "      <td>74.087194</td>\n",
       "      <td>48.603222</td>\n",
       "      <td>56.316379</td>\n",
       "      <td>26.942528</td>\n",
       "      <td>...</td>\n",
       "      <td>207.003862</td>\n",
       "      <td>55.870933</td>\n",
       "      <td>208.442289</td>\n",
       "      <td>55.784846</td>\n",
       "      <td>124.388767</td>\n",
       "      <td>7.205789</td>\n",
       "      <td>133.458801</td>\n",
       "      <td>10.995181</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154.5</td>\n",
       "      <td>196.388049</td>\n",
       "      <td>57.669474</td>\n",
       "      <td>200.363846</td>\n",
       "      <td>50.615080</td>\n",
       "      <td>189.855627</td>\n",
       "      <td>68.516092</td>\n",
       "      <td>63.280031</td>\n",
       "      <td>57.690568</td>\n",
       "      <td>26.164496</td>\n",
       "      <td>...</td>\n",
       "      <td>201.372722</td>\n",
       "      <td>50.922278</td>\n",
       "      <td>203.303091</td>\n",
       "      <td>50.910815</td>\n",
       "      <td>124.570902</td>\n",
       "      <td>7.112593</td>\n",
       "      <td>133.317630</td>\n",
       "      <td>10.926267</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.7</td>\n",
       "      <td>196.586956</td>\n",
       "      <td>56.816229</td>\n",
       "      <td>200.405571</td>\n",
       "      <td>50.129378</td>\n",
       "      <td>190.606173</td>\n",
       "      <td>67.044048</td>\n",
       "      <td>63.870066</td>\n",
       "      <td>57.068227</td>\n",
       "      <td>24.570594</td>\n",
       "      <td>...</td>\n",
       "      <td>201.448391</td>\n",
       "      <td>50.424279</td>\n",
       "      <td>203.433086</td>\n",
       "      <td>50.298129</td>\n",
       "      <td>124.747403</td>\n",
       "      <td>6.862492</td>\n",
       "      <td>132.948486</td>\n",
       "      <td>10.475270</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.4</td>\n",
       "      <td>206.840822</td>\n",
       "      <td>58.391917</td>\n",
       "      <td>210.816657</td>\n",
       "      <td>51.437818</td>\n",
       "      <td>201.517561</td>\n",
       "      <td>68.725207</td>\n",
       "      <td>52.674892</td>\n",
       "      <td>54.528216</td>\n",
       "      <td>22.998771</td>\n",
       "      <td>...</td>\n",
       "      <td>211.541725</td>\n",
       "      <td>51.588476</td>\n",
       "      <td>212.927224</td>\n",
       "      <td>51.478033</td>\n",
       "      <td>124.804936</td>\n",
       "      <td>6.758229</td>\n",
       "      <td>132.644228</td>\n",
       "      <td>10.385137</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147.5</td>\n",
       "      <td>207.421014</td>\n",
       "      <td>58.118789</td>\n",
       "      <td>211.122241</td>\n",
       "      <td>51.420555</td>\n",
       "      <td>202.135509</td>\n",
       "      <td>68.412750</td>\n",
       "      <td>53.727779</td>\n",
       "      <td>56.380803</td>\n",
       "      <td>22.434080</td>\n",
       "      <td>...</td>\n",
       "      <td>211.884094</td>\n",
       "      <td>51.545167</td>\n",
       "      <td>213.275613</td>\n",
       "      <td>51.336971</td>\n",
       "      <td>124.952920</td>\n",
       "      <td>6.614468</td>\n",
       "      <td>132.517304</td>\n",
       "      <td>10.255598</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weight      R_Mean      R_Std      G_Mean      G_Std      B_Mean  \\\n",
       "0   158.9  202.031017  63.184791  206.361271  55.761869  195.552566   \n",
       "1   154.5  196.388049  57.669474  200.363846  50.615080  189.855627   \n",
       "2   149.7  196.586956  56.816229  200.405571  50.129378  190.606173   \n",
       "3   148.4  206.840822  58.391917  210.816657  51.437818  201.517561   \n",
       "4   147.5  207.421014  58.118789  211.122241  51.420555  202.135509   \n",
       "\n",
       "       B_Std     H_Mean      H_Std     S_Mean  ...      V_Mean      V_Std  \\\n",
       "0  74.087194  48.603222  56.316379  26.942528  ...  207.003862  55.870933   \n",
       "1  68.516092  63.280031  57.690568  26.164496  ...  201.372722  50.922278   \n",
       "2  67.044048  63.870066  57.068227  24.570594  ...  201.448391  50.424279   \n",
       "3  68.725207  52.674892  54.528216  22.998771  ...  211.541725  51.588476   \n",
       "4  68.412750  53.727779  56.380803  22.434080  ...  211.884094  51.545167   \n",
       "\n",
       "       L_Mean      L_Std      a_Mean     a_Std      b_Mean      b_Std  Day  \\\n",
       "0  208.442289  55.784846  124.388767  7.205789  133.458801  10.995181    0   \n",
       "1  203.303091  50.910815  124.570902  7.112593  133.317630  10.926267    1   \n",
       "2  203.433086  50.298129  124.747403  6.862492  132.948486  10.475270    2   \n",
       "3  212.927224  51.478033  124.804936  6.758229  132.644228  10.385137    3   \n",
       "4  213.275613  51.336971  124.952920  6.614468  132.517304  10.255598    4   \n",
       "\n",
       "  Temp  \n",
       "0    5  \n",
       "1    5  \n",
       "2    5  \n",
       "3    5  \n",
       "4    5  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../materials/process_csv/weight_color_data.csv')\n",
    "df['Day'] = df['Label'].apply(lambda x: x.split('_')[0])\n",
    "df['Temp'] = df['Label'].apply(lambda x: x.split('_')[1])\n",
    "df.drop('Label', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Weight', 'R_Mean', 'R_Std', 'G_Mean', 'G_Std', 'B_Mean', 'B_Std',\n",
       "       'H_Mean', 'H_Std', 'S_Mean', 'S_Std', 'V_Mean', 'V_Std', 'L_Mean',\n",
       "       'L_Std', 'a_Mean', 'a_Std', 'b_Mean', 'b_Std', 'Day', 'Temp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, QuantileRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = {\n",
    "    \"L\": [\"L_Mean\", \"L_Std\"],\n",
    "    \"a\": [\"a_Mean\", \"a_Std\"],\n",
    "    \"b\": [\"b_Mean\", \"b_Std\"],\n",
    "    \"H\": [\"H_Mean\", \"H_Std\"],\n",
    "    \"S\": [\"S_Mean\", \"S_Std\"],\n",
    "    \"V\": [\"V_Mean\", \"V_Std\"],\n",
    "    \"R\": [\"R_Mean\", \"R_Std\"],\n",
    "    \"G\": [\"G_Mean\", \"G_Std\"],\n",
    "    \"B\": [\"B_Mean\", \"B_Std\"],\n",
    "    # \"Day\": [\"Day\"],\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.1),\n",
    "    \"Polynomial Regression\": make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "    \"Elastic Net\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"Huber Regressor\": HuberRegressor(),\n",
    "    \"Quantile Regressor\": QuantileRegressor(quantile=0.5, alpha=0.1),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_features(features_dict):\n",
    "    feature_combinations = {}\n",
    "\n",
    "    feature_groups = list(features_dict.keys())  # [\"L\", \"a\", \"b\", \"H\", ...]\n",
    "    \n",
    "    for i in range(1, len(feature_groups) + 1):\n",
    "        for comb in combinations(feature_groups, i):  # Create feature group combinations\n",
    "            combined_columns = sum([features_dict[key] for key in comb], [])  # Map to actual column names\n",
    "            feature_combinations[\",\".join(comb)] = combined_columns\n",
    "\n",
    "    print(f\"Generated {len(feature_combinations)} feature combinations.\")\n",
    "    return feature_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_orange(df, red_weight=0.8, green_weight=0.2):\n",
    "    \"\"\"\n",
    "    Create a new feature that represents a mixed color (Orange) from Red and Green.\n",
    "    \n",
    "    This function computes both:\n",
    "    - Mixed color (Mean values)\n",
    "    - Mixed color (Standard Deviation)\n",
    "    - Converts the mixed color to LAB and HSV\n",
    "    \"\"\"\n",
    "    # Compute the mixed color mean\n",
    "    df[\"Mixed_RedGreen_Mean\"] = (df[\"R_Mean\"] * red_weight + df[\"G_Mean\"] * green_weight).astype(int)\n",
    "\n",
    "    # Compute the mixed color standard deviation\n",
    "    df[\"Mixed_RedGreen_Std\"] = (df[\"R_Std\"] * red_weight + df[\"G_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    # Convert Mixed RGB to LAB & HSV\n",
    "    mixed_rgb = np.stack([df[\"Mixed_RedGreen_Mean\"], df[\"Mixed_RedGreen_Mean\"], np.zeros_like(df[\"Mixed_RedGreen_Mean\"])], axis=1)\n",
    "    mixed_bgr = np.array(mixed_rgb, dtype=np.uint8)[:, np.newaxis, :]  # Convert to BGR format\n",
    "    \n",
    "    mixed_lab = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2LAB)[:, 0, :]\n",
    "    mixed_hsv = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2HSV)[:, 0, :]\n",
    "\n",
    "    # Add Mean Values\n",
    "    df[\"Mixed_L_Mean\"] = mixed_lab[:, 0]\n",
    "    df[\"Mixed_a_Mean\"] = mixed_lab[:, 1]\n",
    "    df[\"Mixed_b_Mean\"] = mixed_lab[:, 2]\n",
    "\n",
    "    df[\"Mixed_H_Mean\"] = mixed_hsv[:, 0]\n",
    "    df[\"Mixed_S_Mean\"] = mixed_hsv[:, 1]\n",
    "    df[\"Mixed_V_Mean\"] = mixed_hsv[:, 2]\n",
    "\n",
    "    # Add Standard Deviation (Since we're mixing, we use an estimated std based on weighted input stds)\n",
    "    df[\"Mixed_L_Std\"] = (df[\"L_Std\"] * red_weight + df[\"L_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_a_Std\"] = (df[\"a_Std\"] * red_weight + df[\"a_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_b_Std\"] = (df[\"b_Std\"] * red_weight + df[\"b_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    df[\"Mixed_H_Std\"] = (df[\"H_Std\"] * red_weight + df[\"H_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_S_Std\"] = (df[\"S_Std\"] * red_weight + df[\"S_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_V_Std\"] = (df[\"V_Std\"] * red_weight + df[\"V_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "base_features[\"Orange\"] = [\"Mixed_RedGreen_Mean\", \"Mixed_RedGreen_Std\"]\n",
    "base_features[\"Orange_Lab\"] = [\"Mixed_L_Mean\", \"Mixed_a_Mean\", \"Mixed_b_Mean\", \"Mixed_L_Std\", \"Mixed_a_Std\", \"Mixed_b_Std\"]\n",
    "base_features[\"Orange_HSV\"] = [\"Mixed_H_Mean\", \"Mixed_S_Mean\", \"Mixed_V_Mean\", \"Mixed_H_Std\", \"Mixed_S_Std\", \"Mixed_V_Std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_yellow(df, red_weight=0.5, green_weight=0.5):\n",
    "    \"\"\"\n",
    "    Create a new feature that represents a mixed color (Yellow) from Red and Green.\n",
    "    \n",
    "    This function computes:\n",
    "    - Mixed color (Mean values)\n",
    "    - Mixed color (Standard Deviation)\n",
    "    - Converts the mixed color to LAB and HSV\n",
    "    \"\"\"\n",
    "    # Compute the mixed color mean\n",
    "    df[\"Mixed_RedGreenYellow_Mean\"] = (df[\"R_Mean\"] * red_weight + df[\"G_Mean\"] * green_weight).astype(int)\n",
    "\n",
    "    # Compute the mixed color standard deviation\n",
    "    df[\"Mixed_RedGreenYellow_Std\"] = (df[\"R_Std\"] * red_weight + df[\"G_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    # Convert Mixed RGB to LAB & HSV\n",
    "    mixed_rgb = np.stack([df[\"Mixed_RedGreenYellow_Mean\"], df[\"Mixed_RedGreenYellow_Mean\"], np.zeros_like(df[\"Mixed_RedGreenYellow_Mean\"])], axis=1)\n",
    "    mixed_bgr = np.array(mixed_rgb, dtype=np.uint8)[:, np.newaxis, :]  # Convert to BGR format\n",
    "    \n",
    "    mixed_lab = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2LAB)[:, 0, :]\n",
    "    mixed_hsv = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2HSV)[:, 0, :]\n",
    "\n",
    "    # Add Mean Values\n",
    "    df[\"Mixed_Yellow_L_Mean\"] = mixed_lab[:, 0]\n",
    "    df[\"Mixed_Yellow_a_Mean\"] = mixed_lab[:, 1]\n",
    "    df[\"Mixed_Yellow_b_Mean\"] = mixed_lab[:, 2]\n",
    "\n",
    "    df[\"Mixed_Yellow_H_Mean\"] = mixed_hsv[:, 0]\n",
    "    df[\"Mixed_Yellow_S_Mean\"] = mixed_hsv[:, 1]\n",
    "    df[\"Mixed_Yellow_V_Mean\"] = mixed_hsv[:, 2]\n",
    "\n",
    "    # Add Standard Deviation (Since we're mixing, we use an estimated std based on weighted input stds)\n",
    "    df[\"Mixed_Yellow_L_Std\"] = (df[\"L_Std\"] * red_weight + df[\"L_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_a_Std\"] = (df[\"a_Std\"] * red_weight + df[\"a_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_b_Std\"] = (df[\"b_Std\"] * red_weight + df[\"b_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    df[\"Mixed_Yellow_H_Std\"] = (df[\"H_Std\"] * red_weight + df[\"H_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_S_Std\"] = (df[\"S_Std\"] * red_weight + df[\"S_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_V_Std\"] = (df[\"V_Std\"] * red_weight + df[\"V_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    return df\n",
    "base_features[\"Yellow\"] = [\"Mixed_RedGreenYellow_Mean\", \"Mixed_RedGreenYellow_Std\"]\n",
    "base_features[\"Yellow_LAB\"] = [\"Mixed_Yellow_L_Mean\", \"Mixed_Yellow_a_Mean\", \"Mixed_Yellow_b_Mean\", \"Mixed_Yellow_L_Std\", \"Mixed_Yellow_a_Std\", \"Mixed_Yellow_b_Std\"]\n",
    "base_features[\"Yellow_HSV\"] = [\"Mixed_Yellow_H_Mean\", \"Mixed_Yellow_S_Mean\", \"Mixed_Yellow_V_Mean\", \"Mixed_Yellow_H_Std\", \"Mixed_Yellow_S_Std\", \"Mixed_Yellow_V_Std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Creates interaction features (cross-multiplication) to enhance model learning.\n",
    "    \"\"\"\n",
    "    df[\"R_G_Interaction\"] = df[\"R_Mean\"] * df[\"G_Mean\"]\n",
    "    df[\"R_B_Interaction\"] = df[\"R_Mean\"] * df[\"B_Mean\"]\n",
    "    df[\"G_B_Interaction\"] = df[\"G_Mean\"] * df[\"B_Mean\"]\n",
    "    \n",
    "    df[\"L_H_Interaction\"] = df[\"L_Mean\"] * df[\"H_Mean\"]\n",
    "    df[\"a_S_Interaction\"] = df[\"a_Mean\"] * df[\"S_Mean\"]\n",
    "    df[\"b_V_Interaction\"] = df[\"b_Mean\"] * df[\"V_Mean\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_features(X, degree=2):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    return pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Train and Evaluate Models\n",
    "def train_and_evaluate(X, y, feature_sets, models, output_csv_path):\n",
    "    results = []\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # âœ… Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "    for feature_name, feature_list in feature_sets.items():\n",
    "        X_train_subset = X_train[feature_list]\n",
    "        X_test_subset = X_test[feature_list]\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train_subset, y_train)\n",
    "            y_pred = model.predict(X_test_subset)\n",
    "\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Feature Set\": feature_name,\n",
    "                \"Model\": model_name,\n",
    "                \"MSE\": mse,\n",
    "                \"R2 Score\": r2\n",
    "            })\n",
    "\n",
    "            logging.info(f\"Features: {feature_name}, Model: {model_name}, MSE: {mse:.4f}, R2 Score: {r2:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Regression evaluation complete. Results saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Load Data\n",
    "df = mix_orange(df)  # Apply color mixing\n",
    "df = mix_yellow(df)\n",
    "df = create_interaction_features(df)  # Add interaction features\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = df[sum(base_features.values(), [])]\n",
    "y = df[\"Weight\"]\n",
    "\n",
    "# Train & Evaluate Models\n",
    "def driver_func():\n",
    "    PROCESSES = 4\n",
    "    with multiprocessing.Pool(PROCESSES) as pool:\n",
    "        train_and_evaluate(X, y, progressive_features(base_features), models, \"../output/train_csv/interact.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528548b",
   "metadata": {},
   "source": [
    "## Feature Engineering: Interaction Terms & Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8e5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Creates interaction features (cross-multiplication) to enhance model learning.\n",
    "    \"\"\"\n",
    "    df[\"R_G_Interaction\"] = df[\"R_Mean\"] * df[\"G_Mean\"]\n",
    "    df[\"R_B_Interaction\"] = df[\"R_Mean\"] * df[\"B_Mean\"]\n",
    "    df[\"G_B_Interaction\"] = df[\"G_Mean\"] * df[\"B_Mean\"]\n",
    "    \n",
    "    df[\"L_H_Interaction\"] = df[\"L_Mean\"] * df[\"H_Mean\"]\n",
    "    df[\"a_S_Interaction\"] = df[\"a_Mean\"] * df[\"S_Mean\"]\n",
    "    df[\"b_V_Interaction\"] = df[\"b_Mean\"] * df[\"V_Mean\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Feature Engineering\n",
    "df = create_interaction_features(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f44e4d",
   "metadata": {},
   "source": [
    "## Feature Selection: Selecting Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f256fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Top Features: ['Temp', 'R_Std', 'B_Std', 'L_Std', 'V_Std', 'R_Mean', 'L_Mean', 'R_B_Interaction', 'G_B_Interaction', 'R_G_Interaction', 'G_Std', 'B_Mean', 'a_Mean', 'S_Mean', 'a_S_Interaction']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def select_top_features(X, y, top_n=15):\n",
    "    \"\"\"\n",
    "    Selects top N features using RandomForest feature importance.\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    feature_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    top_features = feature_importance.nlargest(top_n).index.tolist()\n",
    "    \n",
    "    print(\"Selected Top Features:\", top_features)\n",
    "    return X[top_features]\n",
    "\n",
    "# Select top 15 features\n",
    "X = df.drop(columns=['Weight'])\n",
    "y = df[\"Weight\"]\n",
    "X = select_top_features(X, y, top_n=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d3b9a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a04a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best RÂ² Score: 0.780801178307256\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 135 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n135 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 738, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1143, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 603, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n                    ^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1065, in _create_dmatrix\n    return QuantileDMatrix(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 738, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 1585, in __init__\n    self._init(\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 1644, in _init\n    it.reraise()\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 581, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 562, in _handle_exception\n    return fn()\n           ^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 649, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n                                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 1402, in next\n    input_data(**self.kwargs)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 738, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 629, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n                                                   ^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 1447, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 603, in _transform_pandas_df\n    pandas_check_dtypes(data, enable_categorical)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 569, in pandas_check_dtypes\n    _invalid_dataframe_dtype(data)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 356, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Temp: object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Run Hyperparameter Tuning\u001b[39;00m\n\u001b[1;32m     28\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m optimize_model(RandomForestRegressor(), rf_params, X_train, y_train)\n\u001b[0;32m---> 29\u001b[0m best_xgb \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXGBRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m best_ridge \u001b[38;5;241m=\u001b[39m optimize_model(Ridge(), ridge_params, X_train, y_train)\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model, param_grid, X_train, y_train)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mPerforms Grid Search Cross-Validation to find the best hyperparameters.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest RÂ² Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    999\u001b[0m     )\n\u001b[0;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 135 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n135 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 738, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1143, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 603, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n                    ^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1065, in _create_dmatrix\n    return QuantileDMatrix(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 738, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 1585, in __init__\n    self._init(\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 1644, in _init\n    it.reraise()\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 581, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 562, in _handle_exception\n    return fn()\n           ^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 649, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n                                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 1402, in next\n    input_data(**self.kwargs)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 738, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/core.py\", line 629, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n                                                   ^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 1447, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n                                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 603, in _transform_pandas_df\n    pandas_check_dtypes(data, enable_categorical)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 569, in pandas_check_dtypes\n    _invalid_dataframe_dtype(data)\n  File \"/home/j/Desktop/SeniorProject/kale_venv/lib/python3.12/site-packages/xgboost/data.py\", line 356, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Temp: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def optimize_model(model, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Performs Grid Search Cross-Validation to find the best hyperparameters.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"r2\", n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best RÂ² Score:\", grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grids\n",
    "rf_params = {\"n_estimators\": [100, 200, 300], \"max_depth\": [10, 20, None], \"min_samples_split\": [2, 5, 10]}\n",
    "xgb_params = {\"n_estimators\": [100, 200, 300], \"learning_rate\": [0.01, 0.05, 0.1], \"max_depth\": [3, 5, 7]}\n",
    "ridge_params = {\"alpha\": [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "# Run Hyperparameter Tuning\n",
    "best_rf = optimize_model(RandomForestRegressor(), rf_params, X_train, y_train)\n",
    "best_xgb = optimize_model(XGBRegressor(), xgb_params, X_train, y_train)\n",
    "best_ridge = optimize_model(Ridge(), ridge_params, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded04c4b",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e94ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return r2\n",
    "\n",
    "# Evaluate Models\n",
    "evaluate_model(best_rf, X_test, y_test)\n",
    "evaluate_model(best_xgb, X_test, y_test)\n",
    "evaluate_model(best_ridge, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kale_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
