{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load data | Day_Treatment_Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>R_Mean</th>\n",
       "      <th>R_Std</th>\n",
       "      <th>G_Mean</th>\n",
       "      <th>G_Std</th>\n",
       "      <th>B_Mean</th>\n",
       "      <th>B_Std</th>\n",
       "      <th>H_Mean</th>\n",
       "      <th>H_Std</th>\n",
       "      <th>S_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>V_Mean</th>\n",
       "      <th>V_Std</th>\n",
       "      <th>L_Mean</th>\n",
       "      <th>L_Std</th>\n",
       "      <th>a_Mean</th>\n",
       "      <th>a_Std</th>\n",
       "      <th>b_Mean</th>\n",
       "      <th>b_Std</th>\n",
       "      <th>Day</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158.9</td>\n",
       "      <td>202.031017</td>\n",
       "      <td>63.184791</td>\n",
       "      <td>206.361271</td>\n",
       "      <td>55.761869</td>\n",
       "      <td>195.552566</td>\n",
       "      <td>74.087194</td>\n",
       "      <td>48.603222</td>\n",
       "      <td>56.316379</td>\n",
       "      <td>26.942528</td>\n",
       "      <td>...</td>\n",
       "      <td>207.003862</td>\n",
       "      <td>55.870933</td>\n",
       "      <td>208.442289</td>\n",
       "      <td>55.784846</td>\n",
       "      <td>124.388767</td>\n",
       "      <td>7.205789</td>\n",
       "      <td>133.458801</td>\n",
       "      <td>10.995181</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154.5</td>\n",
       "      <td>196.388049</td>\n",
       "      <td>57.669474</td>\n",
       "      <td>200.363846</td>\n",
       "      <td>50.615080</td>\n",
       "      <td>189.855627</td>\n",
       "      <td>68.516092</td>\n",
       "      <td>63.280031</td>\n",
       "      <td>57.690568</td>\n",
       "      <td>26.164496</td>\n",
       "      <td>...</td>\n",
       "      <td>201.372722</td>\n",
       "      <td>50.922278</td>\n",
       "      <td>203.303091</td>\n",
       "      <td>50.910815</td>\n",
       "      <td>124.570902</td>\n",
       "      <td>7.112593</td>\n",
       "      <td>133.317630</td>\n",
       "      <td>10.926267</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.7</td>\n",
       "      <td>196.586956</td>\n",
       "      <td>56.816229</td>\n",
       "      <td>200.405571</td>\n",
       "      <td>50.129378</td>\n",
       "      <td>190.606173</td>\n",
       "      <td>67.044048</td>\n",
       "      <td>63.870066</td>\n",
       "      <td>57.068227</td>\n",
       "      <td>24.570594</td>\n",
       "      <td>...</td>\n",
       "      <td>201.448391</td>\n",
       "      <td>50.424279</td>\n",
       "      <td>203.433086</td>\n",
       "      <td>50.298129</td>\n",
       "      <td>124.747403</td>\n",
       "      <td>6.862492</td>\n",
       "      <td>132.948486</td>\n",
       "      <td>10.475270</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.4</td>\n",
       "      <td>206.840822</td>\n",
       "      <td>58.391917</td>\n",
       "      <td>210.816657</td>\n",
       "      <td>51.437818</td>\n",
       "      <td>201.517561</td>\n",
       "      <td>68.725207</td>\n",
       "      <td>52.674892</td>\n",
       "      <td>54.528216</td>\n",
       "      <td>22.998771</td>\n",
       "      <td>...</td>\n",
       "      <td>211.541725</td>\n",
       "      <td>51.588476</td>\n",
       "      <td>212.927224</td>\n",
       "      <td>51.478033</td>\n",
       "      <td>124.804936</td>\n",
       "      <td>6.758229</td>\n",
       "      <td>132.644228</td>\n",
       "      <td>10.385137</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147.5</td>\n",
       "      <td>207.421014</td>\n",
       "      <td>58.118789</td>\n",
       "      <td>211.122241</td>\n",
       "      <td>51.420555</td>\n",
       "      <td>202.135509</td>\n",
       "      <td>68.412750</td>\n",
       "      <td>53.727779</td>\n",
       "      <td>56.380803</td>\n",
       "      <td>22.434080</td>\n",
       "      <td>...</td>\n",
       "      <td>211.884094</td>\n",
       "      <td>51.545167</td>\n",
       "      <td>213.275613</td>\n",
       "      <td>51.336971</td>\n",
       "      <td>124.952920</td>\n",
       "      <td>6.614468</td>\n",
       "      <td>132.517304</td>\n",
       "      <td>10.255598</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weight      R_Mean      R_Std      G_Mean      G_Std      B_Mean  \\\n",
       "0   158.9  202.031017  63.184791  206.361271  55.761869  195.552566   \n",
       "1   154.5  196.388049  57.669474  200.363846  50.615080  189.855627   \n",
       "2   149.7  196.586956  56.816229  200.405571  50.129378  190.606173   \n",
       "3   148.4  206.840822  58.391917  210.816657  51.437818  201.517561   \n",
       "4   147.5  207.421014  58.118789  211.122241  51.420555  202.135509   \n",
       "\n",
       "       B_Std     H_Mean      H_Std     S_Mean  ...      V_Mean      V_Std  \\\n",
       "0  74.087194  48.603222  56.316379  26.942528  ...  207.003862  55.870933   \n",
       "1  68.516092  63.280031  57.690568  26.164496  ...  201.372722  50.922278   \n",
       "2  67.044048  63.870066  57.068227  24.570594  ...  201.448391  50.424279   \n",
       "3  68.725207  52.674892  54.528216  22.998771  ...  211.541725  51.588476   \n",
       "4  68.412750  53.727779  56.380803  22.434080  ...  211.884094  51.545167   \n",
       "\n",
       "       L_Mean      L_Std      a_Mean     a_Std      b_Mean      b_Std  Day  \\\n",
       "0  208.442289  55.784846  124.388767  7.205789  133.458801  10.995181    0   \n",
       "1  203.303091  50.910815  124.570902  7.112593  133.317630  10.926267    1   \n",
       "2  203.433086  50.298129  124.747403  6.862492  132.948486  10.475270    2   \n",
       "3  212.927224  51.478033  124.804936  6.758229  132.644228  10.385137    3   \n",
       "4  213.275613  51.336971  124.952920  6.614468  132.517304  10.255598    4   \n",
       "\n",
       "  Temp  \n",
       "0    5  \n",
       "1    5  \n",
       "2    5  \n",
       "3    5  \n",
       "4    5  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../materials/process_csv/weight_color_data.csv')\n",
    "df['Day'] = df['Label'].apply(lambda x: x.split('_')[0])\n",
    "df['Temp'] = df['Label'].apply(lambda x: x.split('_')[1])\n",
    "df.drop('Label', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Weight', 'R_Mean', 'R_Std', 'G_Mean', 'G_Std', 'B_Mean', 'B_Std',\n",
       "       'H_Mean', 'H_Std', 'S_Mean', 'S_Std', 'V_Mean', 'V_Std', 'L_Mean',\n",
       "       'L_Std', 'a_Mean', 'a_Std', 'b_Mean', 'b_Std', 'Day', 'Temp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, QuantileRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_features = {\n",
    "    \"L\": [\"L_Mean\", \"L_Std\"],\n",
    "    \"a\": [\"a_Mean\", \"a_Std\"],\n",
    "    \"b\": [\"b_Mean\", \"b_Std\"],\n",
    "    \"H\": [\"H_Mean\", \"H_Std\"],\n",
    "    \"S\": [\"S_Mean\", \"S_Std\"],\n",
    "    \"V\": [\"V_Mean\", \"V_Std\"],\n",
    "    \"R\": [\"R_Mean\", \"R_Std\"],\n",
    "    \"G\": [\"G_Mean\", \"G_Std\"],\n",
    "    \"B\": [\"B_Mean\", \"B_Std\"],\n",
    "    # \"Day\": [\"Day\"],\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.1),\n",
    "    \"Polynomial Regression\": make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "    \"Elastic Net\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"Huber Regressor\": HuberRegressor(),\n",
    "    \"Quantile Regressor\": QuantileRegressor(quantile=0.5, alpha=0.1),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42, verbose=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_features(features_dict):\n",
    "    feature_combinations = {}\n",
    "\n",
    "    feature_groups = list(features_dict.keys())  # [\"L\", \"a\", \"b\", \"H\", ...]\n",
    "    \n",
    "    for i in range(1, len(feature_groups) + 1):\n",
    "        for comb in combinations(feature_groups, i):  # Create feature group combinations\n",
    "            combined_columns = sum([features_dict[key] for key in comb], [])  # Map to actual column names\n",
    "            feature_combinations[\",\".join(comb)] = combined_columns\n",
    "\n",
    "    print(f\"Generated {len(feature_combinations)} feature combinations.\")\n",
    "    return feature_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_orange(df, red_weight=0.8, green_weight=0.2):\n",
    "    \"\"\"\n",
    "    Create a new feature that represents a mixed color (Orange) from Red and Green.\n",
    "    \n",
    "    This function computes both:\n",
    "    - Mixed color (Mean values)\n",
    "    - Mixed color (Standard Deviation)\n",
    "    - Converts the mixed color to LAB and HSV\n",
    "    \"\"\"\n",
    "    # Compute the mixed color mean\n",
    "    df[\"Mixed_RedGreen_Mean\"] = (df[\"R_Mean\"] * red_weight + df[\"G_Mean\"] * green_weight).astype(int)\n",
    "\n",
    "    # Compute the mixed color standard deviation\n",
    "    df[\"Mixed_RedGreen_Std\"] = (df[\"R_Std\"] * red_weight + df[\"G_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    # Convert Mixed RGB to LAB & HSV\n",
    "    mixed_rgb = np.stack([df[\"Mixed_RedGreen_Mean\"], df[\"Mixed_RedGreen_Mean\"], np.zeros_like(df[\"Mixed_RedGreen_Mean\"])], axis=1)\n",
    "    mixed_bgr = np.array(mixed_rgb, dtype=np.uint8)[:, np.newaxis, :]  # Convert to BGR format\n",
    "    \n",
    "    mixed_lab = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2LAB)[:, 0, :]\n",
    "    mixed_hsv = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2HSV)[:, 0, :]\n",
    "\n",
    "    # Add Mean Values\n",
    "    df[\"Mixed_L_Mean\"] = mixed_lab[:, 0]\n",
    "    df[\"Mixed_a_Mean\"] = mixed_lab[:, 1]\n",
    "    df[\"Mixed_b_Mean\"] = mixed_lab[:, 2]\n",
    "\n",
    "    df[\"Mixed_H_Mean\"] = mixed_hsv[:, 0]\n",
    "    df[\"Mixed_S_Mean\"] = mixed_hsv[:, 1]\n",
    "    df[\"Mixed_V_Mean\"] = mixed_hsv[:, 2]\n",
    "\n",
    "    # Add Standard Deviation (Since we're mixing, we use an estimated std based on weighted input stds)\n",
    "    df[\"Mixed_L_Std\"] = (df[\"L_Std\"] * red_weight + df[\"L_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_a_Std\"] = (df[\"a_Std\"] * red_weight + df[\"a_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_b_Std\"] = (df[\"b_Std\"] * red_weight + df[\"b_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    df[\"Mixed_H_Std\"] = (df[\"H_Std\"] * red_weight + df[\"H_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_S_Std\"] = (df[\"S_Std\"] * red_weight + df[\"S_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_V_Std\"] = (df[\"V_Std\"] * red_weight + df[\"V_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "base_features[\"Orange\"] = [\"Mixed_RedGreen_Mean\", \"Mixed_RedGreen_Std\"]\n",
    "base_features[\"Orange_Lab\"] = [\"Mixed_L_Mean\", \"Mixed_a_Mean\", \"Mixed_b_Mean\", \"Mixed_L_Std\", \"Mixed_a_Std\", \"Mixed_b_Std\"]\n",
    "base_features[\"Orange_HSV\"] = [\"Mixed_H_Mean\", \"Mixed_S_Mean\", \"Mixed_V_Mean\", \"Mixed_H_Std\", \"Mixed_S_Std\", \"Mixed_V_Std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_yellow(df, red_weight=0.5, green_weight=0.5):\n",
    "    \"\"\"\n",
    "    Create a new feature that represents a mixed color (Yellow) from Red and Green.\n",
    "    \n",
    "    This function computes:\n",
    "    - Mixed color (Mean values)\n",
    "    - Mixed color (Standard Deviation)\n",
    "    - Converts the mixed color to LAB and HSV\n",
    "    \"\"\"\n",
    "    # Compute the mixed color mean\n",
    "    df[\"Mixed_RedGreenYellow_Mean\"] = (df[\"R_Mean\"] * red_weight + df[\"G_Mean\"] * green_weight).astype(int)\n",
    "\n",
    "    # Compute the mixed color standard deviation\n",
    "    df[\"Mixed_RedGreenYellow_Std\"] = (df[\"R_Std\"] * red_weight + df[\"G_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    # Convert Mixed RGB to LAB & HSV\n",
    "    mixed_rgb = np.stack([df[\"Mixed_RedGreenYellow_Mean\"], df[\"Mixed_RedGreenYellow_Mean\"], np.zeros_like(df[\"Mixed_RedGreenYellow_Mean\"])], axis=1)\n",
    "    mixed_bgr = np.array(mixed_rgb, dtype=np.uint8)[:, np.newaxis, :]  # Convert to BGR format\n",
    "    \n",
    "    mixed_lab = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2LAB)[:, 0, :]\n",
    "    mixed_hsv = cv2.cvtColor(mixed_bgr, cv2.COLOR_RGB2HSV)[:, 0, :]\n",
    "\n",
    "    # Add Mean Values\n",
    "    df[\"Mixed_Yellow_L_Mean\"] = mixed_lab[:, 0]\n",
    "    df[\"Mixed_Yellow_a_Mean\"] = mixed_lab[:, 1]\n",
    "    df[\"Mixed_Yellow_b_Mean\"] = mixed_lab[:, 2]\n",
    "\n",
    "    df[\"Mixed_Yellow_H_Mean\"] = mixed_hsv[:, 0]\n",
    "    df[\"Mixed_Yellow_S_Mean\"] = mixed_hsv[:, 1]\n",
    "    df[\"Mixed_Yellow_V_Mean\"] = mixed_hsv[:, 2]\n",
    "\n",
    "    # Add Standard Deviation (Since we're mixing, we use an estimated std based on weighted input stds)\n",
    "    df[\"Mixed_Yellow_L_Std\"] = (df[\"L_Std\"] * red_weight + df[\"L_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_a_Std\"] = (df[\"a_Std\"] * red_weight + df[\"a_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_b_Std\"] = (df[\"b_Std\"] * red_weight + df[\"b_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    df[\"Mixed_Yellow_H_Std\"] = (df[\"H_Std\"] * red_weight + df[\"H_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_S_Std\"] = (df[\"S_Std\"] * red_weight + df[\"S_Std\"] * green_weight).astype(int)\n",
    "    df[\"Mixed_Yellow_V_Std\"] = (df[\"V_Std\"] * red_weight + df[\"V_Std\"] * green_weight).astype(int)\n",
    "\n",
    "    return df\n",
    "base_features[\"Yellow\"] = [\"Mixed_RedGreenYellow_Mean\", \"Mixed_RedGreenYellow_Std\"]\n",
    "base_features[\"Yellow_LAB\"] = [\"Mixed_Yellow_L_Mean\", \"Mixed_Yellow_a_Mean\", \"Mixed_Yellow_b_Mean\", \"Mixed_Yellow_L_Std\", \"Mixed_Yellow_a_Std\", \"Mixed_Yellow_b_Std\"]\n",
    "base_features[\"Yellow_HSV\"] = [\"Mixed_Yellow_H_Mean\", \"Mixed_Yellow_S_Mean\", \"Mixed_Yellow_V_Mean\", \"Mixed_Yellow_H_Std\", \"Mixed_Yellow_S_Std\", \"Mixed_Yellow_V_Std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Creates interaction features (cross-multiplication) to enhance model learning.\n",
    "    \"\"\"\n",
    "    df[\"R_G_Interaction\"] = df[\"R_Mean\"] * df[\"G_Mean\"]\n",
    "    df[\"R_B_Interaction\"] = df[\"R_Mean\"] * df[\"B_Mean\"]\n",
    "    df[\"G_B_Interaction\"] = df[\"G_Mean\"] * df[\"B_Mean\"]\n",
    "    \n",
    "    df[\"L_H_Interaction\"] = df[\"L_Mean\"] * df[\"H_Mean\"]\n",
    "    df[\"a_S_Interaction\"] = df[\"a_Mean\"] * df[\"S_Mean\"]\n",
    "    df[\"b_V_Interaction\"] = df[\"b_Mean\"] * df[\"V_Mean\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_features(X, degree=2):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    return pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SingleLineLogger:\n",
    "    def __init__(self):\n",
    "        self.last_msg = ''\n",
    "    \n",
    "    def write(self, msg):\n",
    "        # Clear the last message by writing spaces\n",
    "        if self.last_msg:\n",
    "            sys.stdout.write('\\r' + ' ' * len(self.last_msg) + '\\r')\n",
    "        # Write the new message\n",
    "        sys.stdout.write('\\r' + msg)\n",
    "        sys.stdout.flush()\n",
    "        self.last_msg = msg\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "class VSCodeFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        # Simplified format for VS Code output\n",
    "        return f\"{record.levelname}: {record.getMessage()}\"\n",
    "\n",
    "def setup_logging():\n",
    "    # Create logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Remove any existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Create custom stream handler with single line output\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(VSCodeFormatter())\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler for complete logs\n",
    "    file_handler = logging.FileHandler('training.log')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def train_and_evaluate(X, y, feature_sets, models, output_csv_path):\n",
    "    logger = setup_logging()\n",
    "    results = []\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "    # Calculate total steps\n",
    "    total_steps = len(feature_sets) * len(models)\n",
    "\n",
    "    # Custom progress bar format\n",
    "    bar_format = '{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n",
    "    \n",
    "    with tqdm.tqdm(total=total_steps, desc=\"Training Progress\", bar_format=bar_format, \n",
    "              file=sys.stdout, position=0, leave=True) as pbar:\n",
    "        \n",
    "        for feature_name, feature_list in feature_sets.items():\n",
    "            X_train_subset = X_train[feature_list]\n",
    "            X_test_subset = X_test[feature_list]\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                try:\n",
    "                    current_msg = f\"Training {model_name} with {feature_name}\"\n",
    "                    pbar.set_description(current_msg)\n",
    "                    \n",
    "                    # Train and evaluate\n",
    "                    model.fit(X_train_subset, y_train)\n",
    "                    y_pred = model.predict(X_test_subset)\n",
    "\n",
    "                    mse = mean_squared_error(y_test, y_pred)\n",
    "                    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "                    results.append({\n",
    "                        \"Feature Set\": feature_name,\n",
    "                        \"Model\": model_name,\n",
    "                        \"MSE\": mse,\n",
    "                        \"R2 Score\": r2\n",
    "                    })\n",
    "\n",
    "                    logger.info(f\"{feature_name}, {model_name}: MSE={mse:.4f}, R2={r2:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in {model_name} with {feature_name}: {str(e)}\")\n",
    "                \n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "    # Save results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_csv_path, index=False)\n",
    "    logger.info(f\"Training complete. Results saved to {output_csv_path}\")\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32767 feature combinations.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train & Evaluate Models\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogressive_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../output/train_csv/interact.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 70\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(X, y, feature_sets, models, output_csv_path)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Custom progress bar format\u001b[39;00m\n\u001b[1;32m     68\u001b[0m bar_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{desc}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{n_fmt}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{total_fmt}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{elapsed}\u001b[39;00m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{remaining}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Progress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbar_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m          \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_name, feature_list \u001b[38;5;129;01min\u001b[39;00m feature_sets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     74\u001b[0m         X_train_subset \u001b[38;5;241m=\u001b[39m X_train[feature_list]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?"
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ Load Data\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "df = mix_orange(df)  # Apply color mixing\n",
    "df = mix_yellow(df)\n",
    "df = create_interaction_features(df)  # Add interaction features\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = df[sum(base_features.values(), [])]\n",
    "y = df[\"Weight\"]\n",
    "\n",
    "# Train & Evaluate Models\n",
    "with Pool(processes=12) as pool:\n",
    "    train_and_evaluate(X, y, progressive_features(base_features), models, \"../output/train_csv/interact.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kale_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
