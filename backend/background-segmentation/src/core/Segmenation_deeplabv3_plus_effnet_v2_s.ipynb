{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px;\">DeepLabv3+ with EfficientNetv2_S Backbone</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xE82DOmwoHnc",
    "outputId": "9bfc691d-d829-4968-8900-d269c5563b0e"
   },
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/keras-team/keras-cv.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_config(SEED_VALUE):\n",
    "    # Set python `random` seed.\n",
    "    # Set `numpy` seed\n",
    "    # Set `tensorflow` seed.\n",
    "    random.seed(SEED_VALUE)\n",
    "    tf.keras.utils.set_random_seed(SEED_VALUE)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'     \n",
    "    os.environ['TF_USE_CUDNN'] = \"true\"\n",
    "\n",
    "system_config(SEED_VALUE=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPfcc48xoHnc",
    "outputId": "b5fd22f8-df42-4d04-982e-cc9759eede6f"
   },
   "outputs": [],
   "source": [
    "# Download and dataset.\n",
    "def download_and_unzip(url, save_path):\n",
    "\n",
    "    print(\"Downloading and extracting assets...\", end=\"\")\n",
    "    file = requests.get(url)\n",
    "    open(save_path, \"wb\").write(file.content)\n",
    "\n",
    "    try:\n",
    "        # Extract tarfile.\n",
    "        if save_path.endswith(\".zip\"):\n",
    "            with ZipFile(save_path) as zip:\n",
    "                zip.extractall(os.path.split(save_path)[0])\n",
    "\n",
    "        print(\"Done\")\n",
    "    except:\n",
    "        print(\"Invalid file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_URL = r\"https://www.dropbox.com/scl/fi/9k8t9619b4x0hegued5c5/Water-Bodies-Dataset.zip?rlkey=tjgepcai6t74yynmx7tqsm7af&dl=1\"\n",
    "# DATASET_DIR = \"Water-Bodies-Dataset\"\n",
    "# DATASET_ZIP_PATH = os.path.join(os.getcwd(), f\"{DATASET_DIR}.zip\")\n",
    "\n",
    "# # Download if dataset does not exists.\n",
    "# if not os.path.exists(DATASET_DIR):\n",
    "#     download_and_unzip(DATASET_URL, DATASET_ZIP_PATH)\n",
    "#     os.remove(DATASET_ZIP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    IMAGE_SIZE:        tuple = (256, 256)\n",
    "    BATCH_SIZE:          int = 16\n",
    "    NUM_CLASSES:         int = 2\n",
    "    BRIGHTNESS_FACTOR: float = 0.2\n",
    "    CONTRAST_FACTOR:   float = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    MODEL:           str = \"efficientnetv2_s_imagenet\"\n",
    "    EPOCHS:          int = 35\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    CKPT_DIR:        str = os.path.join(\"checkpoints_\"+\"_\".join(MODEL.split(\"_\")[:2]), \n",
    "                                        \"deeplabv3_plus_\"+\"_\".join(MODEL.split(\"_\")[:2])+\".h5\")\n",
    "    LOGS_DIR:        str = \"logs_\"+\"_\".join(MODEL.split(\"_\")[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainingConfig()\n",
    "data_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_images = glob.glob(os.path.join(DATASET_DIR, \"Images\", \"*.jpg\"))\n",
    "# data_masks = glob.glob(os.path.join(DATASET_DIR, \"Masks\", \"*.jpg\"))\n",
    "\n",
    "data_images = \"././dataset/images\"\n",
    "data_masks = \"././dataset/masks\"\n",
    "\n",
    "# Shuffle the data paths before data preparation.\n",
    "zipped_data = list(zip(data_images, data_masks))\n",
    "random.shuffle(zipped_data)\n",
    "data_images, data_masks = zip(*zipped_data)\n",
    "data_images = list(data_images)\n",
    "data_masks = list(data_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "org_data = tf.data.Dataset.from_tensor_slices((data_images, data_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are maintaining a **95-5** split ratio for training and validation samples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.05\n",
    "# Determine the number of validation samples\n",
    "NUM_VAL = int(len(data_images) * SPLIT_RATIO)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_data = org_data.skip(NUM_VAL)\n",
    "valid_data = org_data.take(NUM_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train samples: {train_data.cardinality().numpy()}\")\n",
    "print(f\"Validation samples: {valid_data.cardinality().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Threshold Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV9CoLnzoHne"
   },
   "outputs": [],
   "source": [
    "def read_image_mask(image_path, mask=False, size = data_config.IMAGE_SIZE):\n",
    "    image = tf.io.read_file(image_path)\n",
    "\n",
    "    if mask:\n",
    "        image = tf.io.decode_image(image, channels=1)\n",
    "        image.set_shape([None, None, 1])\n",
    "        image = tf.image.resize(images=image, size=size, method = \"bicubic\")\n",
    "\n",
    "        image_mask = tf.zeros_like(image)\n",
    "        cond = image >=200\n",
    "        updates = tf.ones_like(image[cond])\n",
    "        image_mask = tf.tensor_scatter_nd_update(image_mask, tf.where(cond), updates)\n",
    "        image = tf.cast(image_mask, tf.uint8)\n",
    "\n",
    "    else:\n",
    "        image = tf.io.decode_image(image, channels=3)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = tf.image.resize(images=image, size=size, method = \"bicubic\")\n",
    "        image = tf.cast(tf.clip_by_value(image, 0., 255.), tf.float32)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV9CoLnzoHne"
   },
   "outputs": [],
   "source": [
    "def load_data(image_list, mask_list):\n",
    "    image = read_image_mask(image_list)\n",
    "    mask  = read_image_mask(mask_list, mask=True)\n",
    "    return {\"images\":image, \"segmentation_masks\":mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oKi3rXEoHne"
   },
   "outputs": [],
   "source": [
    "train_ds = train_data.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_ds = valid_data.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpGML_phj7qM"
   },
   "source": [
    "### Unpack Images and Segmentation Masks\n",
    "\n",
    "The `unpackage_inputs` is a utility function that is used to unpack the inputs from the\n",
    "dictionary format to a tuple of `(images, segmentation_masks)`. This will be used later\n",
    "on for visualizing the images and segmentation masks and also the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gUPqZ8VoHnf"
   },
   "outputs": [],
   "source": [
    "def unpackage_inputs(inputs):\n",
    "    images = inputs[\"images\"]\n",
    "    segmentation_masks = inputs[\"segmentation_masks\"]\n",
    "    return images, segmentation_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping class IDs to colors.\n",
    "id2color = {\n",
    "    0: (0,  0,    0),    # Background\n",
    "    1: (102, 204, 255),  # Waterbody\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a single channel mask representation to an RGB mask.\n",
    "def num_to_rgb(num_arr, color_map=id2color):\n",
    "    \n",
    "    # single_layer = np.squeeze(num_arr)\n",
    "    output = np.zeros(num_arr.shape[:2]+(3,))\n",
    "    \n",
    "    for k in color_map.keys():\n",
    "        output[num_arr==k] = color_map[k]\n",
    "        \n",
    "    return output.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_Pyr7_2oHnk"
   },
   "outputs": [],
   "source": [
    "# Function to overlay a segmentation map on top of an RGB image.\n",
    "def image_overlay(image, segmented_image):\n",
    "\n",
    "    alpha = 1.0 # Transparency for the original image.\n",
    "    beta  = 0.7 # Transparency for the segmentation map.\n",
    "    gamma = 0.0 # Scalar added to each sum.\n",
    "\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display a few ground truth images along with the corresponding ground truth mask; and have it overlayed on the image input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_and_mask(data_list, title_list, figsize, color_mask=False, color_map=id2color):\n",
    "    \n",
    "    # Create RGB segmentation map from grayscale segmentation map.\n",
    "    rgb_gt_mask = num_to_rgb(data_list[1], color_map=color_map)\n",
    "    mask_to_overlay = rgb_gt_mask\n",
    "\n",
    "    if len(data_list)==3:\n",
    "        rgb_pred_mask = num_to_rgb(data_list[-1], color_map=color_map)\n",
    "        mask_to_overlay = rgb_pred_mask\n",
    "        \n",
    "    # Create the overlayed image.\n",
    "    overlayed_image = image_overlay(data_list[0], mask_to_overlay)\n",
    "    \n",
    "    data_list.append(overlayed_image)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(data_list), figsize=figsize)\n",
    "    \n",
    "    for idx, axis in enumerate(axes.flat):\n",
    "        axis.set_title(title_list[idx])\n",
    "        if title_list[idx] == \"GT Mask\":\n",
    "            if color_mask:\n",
    "                axis.imshow(rgb_gt_mask)\n",
    "            else:\n",
    "                axis.imshow(data_list[1], cmap=\"gray\")\n",
    "\n",
    "        elif title_list[idx] == \"Pred Mask\":\n",
    "            if color_mask:\n",
    "                axis.imshow(rgb_pred_mask)\n",
    "            else:\n",
    "                axis.imshow(data_list[-1], cmap=\"gray\")\n",
    "            \n",
    "        else:\n",
    "            axis.imshow(data_list[idx])\n",
    "            \n",
    "        axis.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_ds = train_ds.map(unpackage_inputs).batch(3)\n",
    "image_batch, mask_batch = next(iter(plot_train_ds.take(1)))\n",
    "\n",
    "titles = [\"GT Image\", \"GT Mask\", \"Overlayed Mask\"]\n",
    "\n",
    "for image, gt_mask in zip(image_batch, mask_batch):\n",
    "\n",
    "    gt_mask = tf.squeeze(gt_mask, axis=-1).numpy()\n",
    "    display_image_and_mask([image.numpy().astype(np.uint8), gt_mask], \n",
    "                           title_list=titles,\n",
    "                           figsize=(16,6),\n",
    "                           color_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlBeuBS_j7qM"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "We will use the following transforms as augmentations:\n",
    "* RandomFlip (default mode is **\"horizontal\"**, with a probability of `0.5`)\n",
    "* Random Brightness\n",
    "* Random Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52SrFdYZoHnf"
   },
   "outputs": [],
   "source": [
    "augment_fn = tf.keras.Sequential(\n",
    "    [\n",
    "        keras_cv.layers.RandomFlip(),\n",
    "        keras_cv.layers.RandomBrightness(factor=data_config.BRIGHTNESS_FACTOR,\n",
    "                                         value_range=(0, 255)),\n",
    "        keras_cv.layers.RandomContrast(factor=data_config.CONTRAST_FACTOR,\n",
    "                                       value_range=(0, 255)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prepare the final training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqN64bEMoHnf"
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "                train_ds.shuffle(data_config.BATCH_SIZE)\n",
    "                .map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(data_config.BATCH_SIZE)\n",
    "                .map(unpackage_inputs)\n",
    "                .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "                valid_ds.batch(data_config.BATCH_SIZE)\n",
    "                .map(unpackage_inputs)\n",
    "                .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCKbHzroj7qM"
   },
   "source": [
    "### Visualize Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "M_Yc-XNGj7qM",
    "outputId": "d2fe1041-a581-40e5-d902-68828b9f8f54"
   },
   "outputs": [],
   "source": [
    "image_batch, aug_mask_batch = next(iter(train_dataset.take(1)))\n",
    "\n",
    "titles = [\"GT Image\", \"GT Mask\", \"Overlayed Mask\"]\n",
    "\n",
    "for idx, (image, gt_mask) in enumerate(zip(image_batch, aug_mask_batch)):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    gt_mask = tf.squeeze(gt_mask, axis=-1).numpy()\n",
    "    display_image_and_mask([image.numpy().astype(np.uint8), gt_mask], \n",
    "                           title_list=titles,\n",
    "                           figsize=(16,6),\n",
    "                           color_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTIk7uYdj7qM"
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "We will use `efficientnetv2_s_imagenet` feature extractor on top of the DeepLabv3 Head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbmNw-WsoHnf",
    "outputId": "3e8ad6e0-8cfa-4be9-9940-5500e174bd21"
   },
   "outputs": [],
   "source": [
    "backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(preset = train_config.MODEL,\n",
    "                                                          input_shape=data_config.IMAGE_SIZE+(3,),\n",
    "                                                          load_weights = True)\n",
    "model = keras_cv.models.segmentation.DeepLabV3Plus(\n",
    "        num_classes=data_config.NUM_CLASSES, backbone=backbone,\n",
    "    )\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection over Union (IoU) is a metric often used in segmentation problems to assess the model's accuracy. It provides a more intuitive basis for accuracy that is not biased by the (unbalanced) percentage of pixels from any particular class. Given two segmentation masks, `A` and `B`, the IoU is defined as follows:\n",
    "\n",
    "$$ \n",
    "IoU = \\frac{|A\\cap B\\hspace{1mm}|}{|A\\cup B\\hspace{1mm}|} \\hspace{2mm}\n",
    "$$\n",
    "\n",
    "When there are multiple classes and inferences, we assess the model's performance by computing the mean IoU.\n",
    "\n",
    "The function below computes the mean IoU that only considers the classes that are present in the ground truth mask or the predicted segmentation map (sometimes referred to as classwise mean IoU). This computation is a better representation of the metric since it only considers the relevant classes. **This is the metric computation we use for mean IoU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGID2WK6j7qN"
   },
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "\n",
    "    # Get total number of classes from model output.\n",
    "    num_classes = y_pred.shape[-1]\n",
    "\n",
    "    y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), num_classes, axis=-1)\n",
    "    y_pred = tf.one_hot(tf.math.argmax(y_pred, axis=-1), num_classes, axis=-1)\n",
    "\n",
    "    # Intersection: |G ∩ P|. Shape: (batch_size, num_classes)\n",
    "    intersection = tf.math.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "\n",
    "    # Total Sum: |G| + |P|. Shape: (batch_size, num_classes)\n",
    "    total = tf.math.reduce_sum(y_true, axis=(1, 2)) + tf.math.reduce_sum(y_pred, axis=(1, 2))\n",
    "\n",
    "    union = total - intersection\n",
    "\n",
    "    is_class_present =  tf.cast(tf.math.not_equal(total, 0), dtype=tf.float32)\n",
    "    num_classes_present = tf.math.reduce_sum(is_class_present, axis=1)\n",
    "\n",
    "    iou = tf.math.divide_no_nan(intersection, union)\n",
    "    iou = tf.math.reduce_sum(iou, axis=1) / num_classes_present\n",
    "\n",
    "    # Compute the mean across the batch axis. Shape: Scalar\n",
    "    mean_iou = tf.math.reduce_mean(iou)\n",
    "\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard and ModelCheckpoint Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cns-1-opoHng"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(\n",
    "    train_config,\n",
    "    monitor=\"val_mean_iou\",\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "):\n",
    "\n",
    "    # Initialize tensorboard callback for logging.\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=train_config.LOGS_DIR,\n",
    "        histogram_freq=20,\n",
    "        write_graph=False,\n",
    "        update_freq=\"epoch\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Update file path if saving best model weights.\n",
    "    if save_weights_only:\n",
    "        checkpoint_filepath = train_config.CKPT_DIR\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=save_weights_only,\n",
    "        monitor=monitor,\n",
    "        mode=mode,\n",
    "        save_best_only=save_best_only,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return [tensorboard_callback, model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19ZGn0Iij7qN"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Now let's create the model, compile and train by calling `model.fit()` using the configurations defined in the `Trainingconfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "# Get callbacks.\n",
    "callbacks = get_callbacks(train_config)\n",
    "\n",
    "# Define Loss.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Compile model.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(train_config.LEARNING_RATE),\n",
    "    loss=loss_fn,\n",
    "    metrics=[\"accuracy\", mean_iou],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5tTZxfYj7qN",
    "outputId": "10a9b752-927f-41af-abfa-e1eaea155d4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch.\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=train_config.EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Fine-tuned Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbAlRo-IoHng"
   },
   "outputs": [],
   "source": [
    "model.load_weights(train_config.CKPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00qSbPU-oHng",
    "outputId": "3a9c489b-3fd7-4df1-deda-c624a2b067b0"
   },
   "outputs": [],
   "source": [
    "evaluate = model.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHWWIb0qj7qN"
   },
   "source": [
    "## Prediction with Fine-tuned Model\n",
    "\n",
    "Now that the model training of DeepLabv3 has been completed, let's test it by making predictions on a few sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvOXAyTuoHnk"
   },
   "outputs": [],
   "source": [
    "def inference(model, dataset, samples_to_plot):\n",
    "\n",
    "    num_batches_to_process = 2\n",
    "    count = 0\n",
    "    stop_plot = False\n",
    "\n",
    "    titles = [\"Image\", \"GT Mask\", \"Pred Mask\", \"Overlayed Prediction\"]\n",
    "\n",
    "    for idx, data in enumerate(dataset):\n",
    "\n",
    "        if stop_plot:\n",
    "            break\n",
    "\n",
    "        batch_img, batch_mask = data[0], data[1]\n",
    "        batch_pred = (model.predict(batch_img)).astype('float32')\n",
    "        batch_pred = batch_pred.argmax(axis=-1)\n",
    "        \n",
    "        batch_img  = batch_img.numpy().astype('uint8')\n",
    "        batch_mask = batch_mask.numpy().squeeze(axis=-1)\n",
    "\n",
    "        for image, mask, pred in zip(batch_img, batch_mask, batch_pred):\n",
    "            count+=1\n",
    "            display_image_and_mask([image, mask, pred],\n",
    "                                  title_list=titles,\n",
    "                                   figsize=(20,8),\n",
    "                                   color_mask=True)\n",
    "            if count >= samples_to_plot:\n",
    "                stop_plot=True\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3CciQILmoHnk",
    "outputId": "b7d9c5e7-57ac-4239-bbd4-4036ec6766be"
   },
   "outputs": [],
   "source": [
    "inference(model, valid_dataset, samples_to_plot=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faG4QNcWoHnk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bg-segment-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
